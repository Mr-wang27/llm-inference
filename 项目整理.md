# 项目描述：
1. Tensor类: 使用TensorWrapper封装原始数据指针，然后再继承Tensor类，Tensor类中主要是一些相关的数据结构，比如形状、数据类型等相关信息，这样做的目的是为了构建一个TensorMap类，方便管理多个不同数据类型的Tensor,这样可以更加灵活的管理推理过
程中不同类型的数据。
2. 同时从粗粒度到细粒度的构建了各个层的weigth类，llama weight-->layer_weight---> attention_weight、ffn_weight---> norm_weight... 
3. 然后自定义了显存分配器，通过封装一个分配器抽象类来提供对外的接口，然后cuda分配器通过维护一个Bigblock和一个Smallblock，减少malloc核free的次数，减少内存碎片。如果申请的内存大于1M，定义为bigblock，然后先到bigblock pool中去寻找适合的，没有找到才使用cudamalloc.如果小于1M，就到smallblock pool中寻找合适的blokc,没有找到也才分配新的。然后，如果累积的smallblokc中的数据量达到了1G， 就考虑释放掉这些小的内存，清理内存碎片。
如果内存非常吃紧，在分配大内存没有的时候，就从bigblokc中清理一定大小的空间，然后在重新尝试分配。
4. cuda算子层就是是实现了llama2的整个前向推理过程：
    - 整个流程是先对输入使用tokenizer编码乘input_id之后，通过embedding kernel，
    - 计算padding offset: 也就是计算每个seq的padding，这样可以在计算前attention，通过padding offset将数据padding到一样的长度，以便在计算attention的时候调用Batch gemm进行计算，然后在attention计算完毕之后，可以再次使用padding offset，将该padding移除，减少后面的ffn层中的计算量。
    - 然后将输入的数据计算RSMNorm kernel， 归一化输入数据。
    - 然后计算再prefill阶段需要的casual mask，以便在后面计算softmax之前，屏蔽未来的token,通过将这些数据全部赋值为负无穷，来使这些数据经过softmax之后便为0.
    - 然后通过linear kernel计算得到QKV,
    - 然后将QK分别计算各自的RoPE, 在这里计算前将qkv加上各自的bias。然后再计算RoPE,然后在数据写回的使用进行padding,得到Q,K,V这三个矩阵
        这个kernel的实现有一些比较坑的地方，就是在hugging face上，旋转的元素是0-64， 1-65... 63-128进行配对旋转的，这就导致在实现的时候没办法使用向量化指令进行优化，除非在RSMNorm kernel之前再使用一个kernel调整数据的布局，然后再进行向量化的存取，但是这样同样会增加依次访存，所以感觉优化不大，所以对于这个kernel并没有进行向量化加速。
        同时
    - 然后将计算完毕的k与v与之前kv cache中的kv进行concat:访存密集型kernel 
    - 然后在计算Q @ K^T之前，未来支持MQA与GQA,将k、v进行对应的repeat
    - 然后使用linear kernel计算Q @ K^T
    - 然后计算maskAndSoftmax kernel： 将之前计算的casaul mask的对应的位置赋值为负无穷，然后再计算每个数据的Scale,然后再计算每行的和(reduce sum操作)，存在共享内存上，然后再计算每个score,最后写回即可
    - 然后再次利用linear gemm(batch gemm)计算Attention score @ v得到最终的结果
    - 然后将计算的attention 结果移除padding
    - 然后将输出的结果与之间经过rmsnorm的residual进行相加，再将相加的结果计算softmax
    - 然后进入ffn层，将gate linear 与 up linear 的权重在外部进行拼接，然后在这里使用一个linear kernel gemm计算(大矩阵乘)
    -
    - 最后再经过激活函数SwiGLU kernel计算，既可以得到输出的一个结果。 然后再加上一个residual就结束了
        自此，一个layer的推理到这里便结束了。

    - 然后进入采样层，先是一个head linear计算：使用linear kernel计算
    - 然后使用topk进行排序，选取topk个数据
    - 最后对着topk个数据进行采样即可。


    - 最后FusedDecoderSelfAfAttention kernel： ConcatPastKVCache--->Broadcast--->QK gemv--->Scale--->Softamx--->QK*v gemv： 融合的算子数量


5. 重点kernel:
    单独算子：
    - Embedding
    - GEMV      (这里的实现就是风速数据处理的实现)
    - RMSNorm

    融合算子：
    - FusedDecoderSelfAttention
    - FusedMaskAndScaleSofrmax
    - fusedBiasRope

6. gemm, flash attention, cuda stream ， 量化， cuda内存层次结构


# allocator设计
1. 采用BaseAllocator封装一个基本的Allocator作为对外的接口，以便后续添加不同memory pool 的管理策略的allocator.
2. 在CPU端对GPU的分配的显存进行管理，管理的数据内存主要分为两个部分：数据权重和kernel计算的中间数据，以及KV Cache的空间
3. 如果是多卡的推理，该allocator会单独管理每张卡的显存，即每张卡均维护两个内存pool:BigBlock与SmallBlock。
4. 采用两个的内存pool，一个BigBlock，一个SmallBlock.
    其中BigBlokc维护大块的内存申请
    SmallBlock维护小块的内存申请。

    大块的内存申请通常并不会造成太多的外部内存碎片问题，但是小块的内存分配就很容易导致外部内存碎片。
    所以使用smallblock来维护一个小block的内存pool，以便收集这些小块内存，然后在free的时候清理这些内存碎片


    因为如果不分开维护，那么在内存pool中，会由于小内存的频繁malloc与free导致出现大量外部内存碎片，导致大块内存申请时无法进行分配。
    因为在推理的场景下，大块内存(Weights和kernel计算的中间结果)的申请经常会存在，同时还有一些小的内存申请也会经常存在。


5. 策略：
    有一个内存申请来临时
    - 5.1 如果申请大于1MB的内存，则在BigBlock中寻找是否右满足条件的内存申请：大于请求的内存同时内部内存碎片小于1MB。如果没有找到则直接通过cudaMalloc进行分配。如果分配失败，则尝试将BigBlock中维护的内存空间进行释放，然后再次尝试进行分配。
     
    - 5.2 如果申请小于1MB的内存，则在SmallBlock中寻找最符合条件的内存块：大于请求的内存，同时内部内存碎片最小。如果没有符合条件的，则直接通过cudaMalloc进行分配。

    释放一个内存时
    - 5.3 如果释放的是大内存，则将其放入到BigBlock中，同时标记该块内存已经被free。但是不将其归还给OS
    - 5.4 对于SmallBlock同理

6. 补充：对于CPU端的内存，并未维护内存pool，而是直接调用对应的malloc与free


# Tensor类
1. 使用模板类TensorWrapper继承Tensor类，TensorWrapper封装数据指针，以便扩展不同数据类型的权重推理流程。同时使用TensorMap类，以便对不同数据类型的Tensor进行统一管理。

# Wigeht类
1. 抽象基类BaseWeight：所有细粒度的权重类(attention_weight、embedding_weights、ffn_weights、norm_weights)全部继承BaseWeight
2. 然后从细粒度的权重类逐层合并，实现layer_weight(attention_weight、embedding_weights、ffn_weights)---->llama weights(layer_weight, norm_weight)
3. 在layer_weights与llama_weights中实现真实权重加载与Dummy权重加载方法。

# layers类
1. 实现相关层的前向推理流程

# llama类
1. 将所有kernel的前向推理流程进行整合





# kernel层(算子整理) + Sgemm + Conv + gemv

## gemv_1: 矩阵向量乘
    1. 每个线程读取float4或者4个half2类型的数据**向量化访存**，将数据读取到寄存器中(或共享内存中)
    2. dot product
    3. reduce（需要将数据放在共享内存上）
    4. 写回显存



